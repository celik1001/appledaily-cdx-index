{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf351da5-b9d7-4689-9bad-165ad31b9f80",
   "metadata": {},
   "source": [
    "# 使用 CDX api 查找 時間範圍內的網址內容\n",
    "\n",
    "## 簡介\n",
    "\n",
    "這支程式的目的是協助使用者獲得特定年月、類別的文章列表，\n",
    "透過 Wayback CDX API 與`requests`取得文章 url、Wayback 快取的網址、文章 id，能夠初步查看總共有哪些文章。\n",
    "\n",
    "為了加快速度，使用了多執行緒來加快程式運作。\n",
    "請先透過命令列或anacoda 安裝jupyter notebook。\n",
    "這支程式會使用 `requests`、`Beautiful Soup`這個套件與 Wayback Machine 做互動。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed5972-3fa6-4034-a351-80be39e8e7bc",
   "metadata": {},
   "source": [
    "## 引入所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0731bdc-a2a9-4098-80e4-22b095eb5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, csv, time, random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6b991-6693-4c1a-ab64-19857e5e49a3",
   "metadata": {},
   "source": [
    "## 定義函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e183bfb-a3f0-4585-9543-8c58d5ad3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loggersetting\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "API = \"https://web.archive.org/cdx/search/cdx\"\n",
    "\n",
    "class MultiCategoryWaybackScraper:\n",
    "    def __init__(self, max_workers=2, max_retries=3, timeout=60):\n",
    "        self.max_workers = max_workers\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"Accept-Encoding\": \"gzip\"})\n",
    "        self.lock = Lock()\n",
    "        self.stats = defaultdict(lambda: {\"success\": 0, \"failed\": 0, \"total_items\": 0})\n",
    "\n",
    "    def fetch_with_retry(self, params, info=\"\"):\n",
    "        \"\"\"帶重試機制的請求\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.session.get(API, params=params, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if not data:\n",
    "                    logger.warning(f\"{info} - Empty response on attempt {attempt + 1}\")\n",
    "                    continue\n",
    "                    \n",
    "                header, *items = data\n",
    "                logger.info(f\"{info} - Success: {len(items)} items\")\n",
    "                return items\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                wait_time = random.uniform(2, 5) * (attempt + 1)\n",
    "                logger.warning(f\"{info} - Attempt {attempt + 1} failed: {e}\")\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    logger.info(f\"{info} - Waiting {wait_time:.1f}s before retry\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"{info} - All attempts failed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"{info} - Unexpected error: {e}\")\n",
    "                break\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def fetch_single_day(self, date, category):\n",
    "        params = {\n",
    "            \"url\": f\"www.appledaily.com.tw/{category}/{date}/*\",\n",
    "            \"output\": \"json\",\n",
    "            \"fl\": \"timestamp,original\",\n",
    "            \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n",
    "            \"collapse\": \"original\",\n",
    "            \"limit\": 5000,\n",
    "        }\n",
    "        \n",
    "        info = f\"[{category}] {date}\"\n",
    "        items = self.fetch_with_retry(params, info)\n",
    "        \n",
    "        with self.lock:\n",
    "            if items:\n",
    "                self.stats[category][\"success\"] += 1\n",
    "                self.stats[category][\"total_items\"] += len(items)\n",
    "            else:\n",
    "                self.stats[category][\"failed\"] += 1\n",
    "        \n",
    "        results = []\n",
    "        for ts, orig in items:\n",
    "            try:\n",
    "                clean = orig.split('?', 1)[0]\n",
    "                article_id = clean.rstrip('/').split('/')[-1]\n",
    "                \n",
    "                if not article_id or article_id == category:\n",
    "                    continue\n",
    "                    \n",
    "                article_url = f\"https://www.appledaily.com.tw/{category}/{date}/{article_id}/\"\n",
    "                results.append((date, article_id, article_url, category))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"{info} - Error processing item {orig}: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def fetch_category_data(self, months, category):\n",
    "        \"\"\"fetch_single category_data\"\"\"\n",
    "        logger.info(f\"Starting category [{category}] for months: {months}\")\n",
    "        category_start = time.time()\n",
    "        \n",
    "        # generate all dates\n",
    "        all_dates = []\n",
    "        for month in months:\n",
    "            for day in range(1, 32):\n",
    "                all_dates.append(f\"{month}{day:02d}\")\n",
    "        \n",
    "        all_rows = []\n",
    "        failed_requests = []\n",
    "        \n",
    "        # multithreads\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_info = {\n",
    "                executor.submit(self.fetch_single_day, date, category): (date, category)\n",
    "                for date in all_dates\n",
    "            }\n",
    "            \n",
    "            completed = 0\n",
    "            total = len(future_to_info)\n",
    "            \n",
    "            for future in as_completed(future_to_info):\n",
    "                date, cat = future_to_info[future]\n",
    "                completed += 1\n",
    "                \n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    if results:\n",
    "                        all_rows.extend(results)\n",
    "                    else:\n",
    "                        failed_requests.append((date, cat))\n",
    "                        \n",
    "                    # 每完成50個請求顯示進度\n",
    "                    if completed % 50 == 0:\n",
    "                        logger.info(f\"[{category}] Progress: {completed}/{total} ({completed/total*100:.1f}%)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"[{category}] {date} - Future exception: {e}\")\n",
    "                    failed_requests.append((date, cat))\n",
    "        \n",
    "        # retryfailed request\n",
    "        if failed_requests:\n",
    "            logger.info(f\"[{category}] Retrying {len(failed_requests)} failed requests\")\n",
    "            retry_count = 0\n",
    "            for date, cat in failed_requests[:]:  # 複製列表避免修改問題\n",
    "                time.sleep(random.uniform(3, 6))  # set 重試間隔更長\n",
    "                retry_results = self.fetch_single_day(date, cat)\n",
    "                if retry_results:\n",
    "                    all_rows.extend(retry_results)\n",
    "                    failed_requests.remove((date, cat))\n",
    "                    retry_count += 1\n",
    "                    \n",
    "                # 每重試10個顯示進度\n",
    "                if retry_count % 10 == 0:\n",
    "                    logger.info(f\"[{category}] Retry progress: {retry_count}\")\n",
    "        \n",
    "        category_elapsed = time.time() - category_start\n",
    "        logger.info(f\"[{category}] Completed in {category_elapsed:.2f}s, \"\n",
    "                   f\"got {len(all_rows)} articles, {len(failed_requests)} failed\")\n",
    "        \n",
    "        return all_rows, failed_requests\n",
    "\n",
    "    def save_category_to_csv(self, rows, category, months):\n",
    "        \"\"\"save_category_to_csv\"\"\"\n",
    "        if not rows:\n",
    "            logger.warning(f\"[{category}] No data to save\")\n",
    "            return 0\n",
    "            \n",
    "        # unique\n",
    "        unique_articles = {}\n",
    "        for date, article_id, url, cat in rows:\n",
    "            key = f\"{cat}_{article_id}\"\n",
    "            if key not in unique_articles:\n",
    "                unique_articles[key] = (date, article_id, url, cat)\n",
    "            else:\n",
    "                existing_date = unique_articles[key][0]\n",
    "                if date < existing_date:\n",
    "                    unique_articles[key] = (date, article_id, url, cat)\n",
    "        \n",
    "        # sort and write\n",
    "        sorted_articles = sorted(unique_articles.values())\n",
    "        \n",
    "        month_range = f\"{months[0]}-{months[-1]}\"\n",
    "        filename = f\"appledaily_{category}_{month_range}.csv\"\n",
    "        \n",
    "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"date\", \"article_id\", \"url\", \"category\"])\n",
    "            for row in sorted_articles:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        logger.info(f\"[{category}] Saved {len(sorted_articles)} unique articles to {filename}\")\n",
    "        return len(sorted_articles), filename\n",
    "\n",
    "    def save_all_categories_to_csv(self, all_data, months):\n",
    "        \"\"\"保存所有分類的資料到一個總CSV\"\"\"\n",
    "        if not all_data:\n",
    "            logger.warning(\"No data to save to combined file\")\n",
    "            return 0, \"\"\n",
    "            \n",
    "        # combine all data and unique\n",
    "        unique_articles = {}\n",
    "        for rows in all_data.values():\n",
    "            for date, article_id, url, category in rows:\n",
    "                key = f\"{category}_{article_id}\"\n",
    "                if key not in unique_articles:\n",
    "                    unique_articles[key] = (date, article_id, url, category)\n",
    "                else:\n",
    "                    existing_date = unique_articles[key][0]\n",
    "                    if date < existing_date:\n",
    "                        unique_articles[key] = (date, article_id, url, category)\n",
    "        \n",
    "        # sort and write\n",
    "        sorted_articles = sorted(unique_articles.values())\n",
    "        \n",
    "        month_range = f\"{months[0]}-{months[-1]}\"\n",
    "        filename = f\"appledaily_ALL_CATEGORIES_{month_range}.csv\"\n",
    "        \n",
    "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"date\", \"article_id\", \"url\", \"category\"])\n",
    "            for row in sorted_articles:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        logger.info(f\"Saved {len(sorted_articles)} unique articles (all categories) to {filename}\")\n",
    "        return len(sorted_articles), filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ce698-8279-43d1-b926-5f6341599fba",
   "metadata": {},
   "source": [
    "## 這裡是主程式\n",
    "可以輸入任一月份、以及新聞各個類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bf9ad-4669-4740-bd84-e9fe55391979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set variables\n",
    "    months = [\"202201\", \"202202\", \"202203\", \"202204\", \"202205\"]\n",
    "    categories = [\n",
    "        \"local\",        # 地方新聞\n",
    "        \"realtime\",     # 即時新聞  \n",
    "        \"entertainment\", # 娛樂新聞\n",
    "        \"sports\",       # 體育新聞\n",
    "        \"international\", # 國際新聞\n",
    "        \"finance\",      # 財經新聞\n",
    "        \"life\",         # 生活新聞\n",
    "        \"forum\"         # 論壇\n",
    "    ]\n",
    "    \n",
    "    # set \n",
    "    scraper = MultiCategoryWaybackScraper(\n",
    "        max_workers=2,      # 2threads\n",
    "        max_retries=3,      # retry 3 times\n",
    "        timeout=60          # 60s timeout\n",
    "    )\n",
    "    \n",
    "    # start scrapping\n",
    "    overall_start = time.time()\n",
    "    all_category_data = {}\n",
    "    category_files = []\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"STARTING MULTI-CATEGORY SCRAPING\")\n",
    "    logger.info(f\"Months: {months}\")\n",
    "    logger.info(f\"Categories: {categories}\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    for i, category in enumerate(categories, 1):\n",
    "        logger.info(f\"\\n{'='*20} CATEGORY {i}/{len(categories)}: {category.upper()} {'='*20}\")\n",
    "        \n",
    "        category_rows, failed = scraper.fetch_category_data(months, category)\n",
    "        all_category_data[category] = category_rows\n",
    "        \n",
    "        # save single\n",
    "        if category_rows:\n",
    "            count, filename = scraper.save_category_to_csv(category_rows, category, months)\n",
    "            category_files.append((category, count, filename))\n",
    "        else:\n",
    "            logger.warning(f\"[{category}] No data found!\")\n",
    "            category_files.append((category, 0, \"No file\"))\n",
    "        \n",
    "        # rest between category\n",
    "        if i < len(categories):\n",
    "            rest_time = random.uniform(30, 60)\n",
    "            logger.info(f\"Resting {rest_time:.1f}s before next category...\")\n",
    "            time.sleep(rest_time)\n",
    "    \n",
    "    # saved total data\n",
    "    total_count, combined_file = scraper.save_all_categories_to_csv(all_category_data, months)\n",
    "    \n",
    "    # final report\n",
    "    total_time = time.time() - overall_start\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 80)\n",
    "    logger.info(\"FINAL REPORT\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    logger.info(f\"Months processed: {', '.join(months)}\")\n",
    "    logger.info(f\"Categories processed: {len(categories)}\")\n",
    "    \n",
    "    logger.info(\"\\nPer-category results:\")\n",
    "    total_articles = 0\n",
    "    for category, count, filename in category_files:\n",
    "        logger.info(f\"  {category:>15}: {count:>6} articles → {filename}\")\n",
    "        total_articles += count\n",
    "    \n",
    "    logger.info(f\"\\nCombined results:\")\n",
    "    logger.info(f\"  Total raw articles: {total_articles}\")\n",
    "    logger.info(f\"  Unique articles: {total_count}\")\n",
    "    logger.info(f\"  Combined file: {combined_file}\")\n",
    "    \n",
    "    logger.info(\"\\nSuccess rates by category:\")\n",
    "    for category in categories:\n",
    "        stats = scraper.stats[category]\n",
    "        if stats[\"success\"] + stats[\"failed\"] > 0:\n",
    "            success_rate = stats[\"success\"] / (stats[\"success\"] + stats[\"failed\"]) * 100\n",
    "            logger.info(f\"  {category:>15}: {success_rate:>5.1f}% \"\n",
    "                       f\"({stats['success']}/{stats['success'] + stats['failed']} requests, \"\n",
    "                       f\"{stats['total_items']} items)\")\n",
    "        else:\n",
    "            logger.info(f\"  {category:>15}: No requests made\")\n",
    "# --------------- scrapping end ------------------\n",
    "    # create  summary_file\n",
    "    summary_file = f\"SUMMARY_appledaily_{months[0]}-{months[-1]}.txt\"\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Apple Daily Archive Summary\\n\")\n",
    "        f.write(f\"Period: {months[0]} - {months[-1]}\\n\")\n",
    "        f.write(f\"Execution time: {total_time:.2f} seconds\\n\")\n",
    "        f.write(f\"Total unique articles: {total_count}\\n\\n\")\n",
    "        f.write(\"Files created:\\n\")\n",
    "        for category, count, filename in category_files:\n",
    "            if filename != \"No file\":\n",
    "                f.write(f\"  {filename}\\n\")\n",
    "        f.write(f\"  {combined_file}\\n\")\n",
    "    \n",
    "    logger.info(f\"\\nSummary saved to: {summary_file}\")\n",
    "    logger.info(\"SCRAPING COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919d2af-b375-4cdf-8e46-d4ab6f0b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa6f89-ffbd-4f90-a992-f480cdf1f1b6",
   "metadata": {},
   "source": [
    "最後會輸出成像這樣的檔案結構\n",
    "./\n",
    "├─ appledaily_local_202201-202205.csv\n",
    "├─ appledaily_realtime_202201-202205.csv\n",
    "├─ appledaily_entertainment_202201-202205.csv\n",
    "├─ appledaily_sports_202201-202205.csv\n",
    "├─ appledaily_international_202201-202205.csv\n",
    "├─ appledaily_finance_202201-202205.csv\n",
    "├─ appledaily_life_202201-202205.csv\n",
    "├─ appledaily_forum_202201-202205.csv\n",
    "├─ appledaily_ALL_CATEGORIES_202201-202205.csv\n",
    "└─ SUMMARY_appledaily_202201-202205.txt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
