{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c273dcc-bc6a-4e5a-813d-b1777fb34b3b",
   "metadata": {},
   "source": [
    "# 使用 BeautifulSoup、requests 來擷取 Wayback Machine 上的網頁內容並存檔\n",
    "\n",
    "## 簡介\n",
    "\n",
    "這支程式的目的是協助使用者擷取特定年月、類別的文章列表，\n",
    "透過 Wayback CDX API 與`requests`、`BeautifulSoup`取得文章 url、Wayback 快取的網址、文章 id，能夠初步查看總共有哪些文章。\n",
    "然後用`pandas`包成CSV。請先安裝這些套件。\n",
    "\n",
    "為了加快速度，使用了多執行緒來加快程式運作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7eea6a-948f-490e-a671-bc9f450b4f79",
   "metadata": {},
   "source": [
    "## 引入程式套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be92e610-d505-4b74-98cb-022ed3b191ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae6a2b-ac6b-4b8d-a8c4-911fec0af0d5",
   "metadata": {},
   "source": [
    "## Basic setting\n",
    "這是主要設定，使用者可以透過既有的文章連結列表，叫出原始的url內容。\n",
    "`INPUT_CSV`: \"example.csv\"  放入檔案路徑 <br>\n",
    "\n",
    "`URL_FIELD` = \"uri\" 新聞文章的原始uri\n",
    "\n",
    "`ID_FIELD` = \"id\" 文章的uid\n",
    "\n",
    "`THREADS` = 5 執行緒的數量\n",
    "\n",
    "`N_LIMIT` = 5    # None == all\n",
    "\n",
    "`OUTPUT_PREFIX` = \"output_\"\n",
    "\n",
    "`SKIP_FILE_NAME` = \"skipped_urls.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7018a3f5-3e06-4f82-98a5-3a2521d49875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setting 這是主要設定\n",
    "INPUT_CSV = \"only_in_b.csv\"\n",
    "URL_FIELD = \"uri\"\n",
    "ID_FIELD = \"id\"\n",
    "THREADS = 5\n",
    "N_LIMIT = 5                  # None == all\n",
    "OUTPUT_PREFIX = \"output_\"\n",
    "SKIP_FILE_NAME = \"skipped_urls.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f54517-0524-4952-bb05-17e5a58419fb",
   "metadata": {},
   "source": [
    "## 處理資料的函式庫\n",
    "`helpers` 幫忙轉換日期格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaf8f9ba-a8ae-462e-96f2-b5342ca6c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers \n",
    "def _only_date(s: str) -> str:\n",
    "    \"\"\"Normalize time string to YYYY-MM-DD. Return '' if parsing fails.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    if s.isdigit() and len(s) == 14:  # CDX: YYYYmmddHHMMSS\n",
    "        try:\n",
    "            return datetime.strptime(s, \"%Y%m%d%H%M%S\").strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S%z\",\n",
    "                \"%Y-%m-%dT%H:%M:%S\",\n",
    "                \"%Y-%m-%d %H:%M:%S\",\n",
    "                \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            dt = datetime.strptime(s.replace(\"Z\", \"+0000\"), fmt)\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    if len(s) >= 10 and s[4] == \"-\" and s[7] == \"-\":\n",
    "        return s[:10]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "252bdf8a-cace-4019-aa82-5f64d7699ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_path_segment(raw_url: str) -> str:\n",
    "    \"\"\"Return first URL path segment (e.g., /local/20200729/... -> local).\"\"\"\n",
    "    try:\n",
    "        p = urlparse(raw_url)\n",
    "        parts = [seg for seg in p.path.split(\"/\") if seg]\n",
    "        return parts[0] if parts else \"\"\n",
    "    except Exception:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c89ff613-ca03-489e-b7fe-e5020867d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_img_url(src: str) -> str:\n",
    "    \"\"\"Normalize to absolute Wayback URL.\"\"\"\n",
    "    if not src:\n",
    "        return \"\"\n",
    "    src = src.strip()\n",
    "    if src.startswith(\"//\"):\n",
    "        return \"https:\" + src\n",
    "    if src.startswith(\"/\"):\n",
    "        return \"https://web.archive.org\" + src\n",
    "    if not src.startswith(\"http\"):\n",
    "        return \"https://web.archive.org\" + (\"/\" + src if not src.startswith(\"/\") else src)\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b0397e6-6476-4b28-acc5-2d285e91f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _content_type_from_ext(filename: str) -> str:\n",
    "    \"\"\"Map file extension to MIME type.\"\"\"\n",
    "    ext = os.path.splitext(filename)[1].lower().lstrip(\".\")\n",
    "    if ext in (\"jpg\", \"jpeg\"):\n",
    "        return \"image/jpeg\"\n",
    "    if ext in (\"png\", \"gif\", \"webp\", \"bmp\", \"svg\"):\n",
    "        return f\"image/{ext}\"\n",
    "    return \"image/jpeg\"\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    \"\"\"Light normalization for pattern matching (unify slashes, remove extra spaces).\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"／\", \"/\").replace(\"　\", \" \").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _extract_by_loc_text(text: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract 'by' (reporter) and 'located' (place/desk) from text such as:\n",
    "      - 記者周庭慶／台中報導\n",
    "      - 地方中心周庭慶／台中報導\n",
    "      - 周庭慶／台中報導\n",
    "      - （…）變體皆可；允許 / 與 ／；報導/報道 都接受\n",
    "    Returns (by, located) or (\"\",\"\") if not found.\n",
    "    \"\"\"\n",
    "    t = _normalize_text(text)\n",
    "\n",
    "    # Common patterns\n",
    "    patterns = [\n",
    "        # Optional prefix + NAME / PLACE 報導|報道\n",
    "        r'(?:記者|地方中心|採訪中心|特派|特約)?\\s*([\\u4e00-\\u9fa5A-Za-z·．\\.\\s]{2,20})\\s*/\\s*([\\u4e00-\\u9fa5A-Za-z·\\.\\s]{1,10})\\s*報[導道]',\n",
    "        # NAME / PLACE 報導|報道\n",
    "        r'([\\u4e00-\\u9fa5A-Za-z·．\\.\\s]{2,20})\\s*/\\s*([\\u4e00-\\u9fa5A-Za-z·\\.\\s]{1,10})\\s*報[導道]',\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, t)\n",
    "        if m:\n",
    "            by = m.group(1).strip(\" ，,。.!?（）()\")\n",
    "            located = m.group(2).strip(\" ，,。.!?（）()\")\n",
    "            return by, located\n",
    "\n",
    "    # Fallback: only \"(PLACE報導)\" without a clear name\n",
    "    m = re.search(r'([\\u4e00-\\u9fa5A-Za-z·\\.\\s]{1,10})\\s*報[導道]', t)\n",
    "    if m:\n",
    "        return \"\", m.group(1).strip(\" ，,。.!?（）()\")\n",
    "\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5692d2da-30ef-4919-9c15-0ee4975056ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wayback Scraper \n",
    "class WaybackScraper:\n",
    "    def __init__(self, img_root_dir):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})\n",
    "        self.img_root_dir = img_root_dir\n",
    "        os.makedirs(self.img_root_dir, exist_ok=True)\n",
    "\n",
    "    def get_latest_snapshot(self, url):\n",
    "        \"\"\"Query CDX for the latest successful snapshot.\"\"\"\n",
    "        api = \"https://web.archive.org/cdx/search/cdx\"\n",
    "        params = {\n",
    "            'url': url,\n",
    "            'output': 'json',\n",
    "            'filter': 'statuscode:200',\n",
    "            'sort': 'reverse',\n",
    "            'limit': 1\n",
    "        }\n",
    "        try:\n",
    "            r = self.session.get(api, params=params, timeout=20)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            if len(data) > 1:\n",
    "                row = data[1]\n",
    "                return {\n",
    "                    'timestamp': row[1],  # YYYYmmddHHMMSS\n",
    "                    'original_url': row[2],\n",
    "                    'wayback_url': f\"https://web.archive.org/web/{row[1]}/{row[2]}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"[CDX error] {url} → {e}\")\n",
    "        return None\n",
    "\n",
    "    def _extract_published_date(self, soup, wayback_ts: str) -> str:\n",
    "        \"\"\"Use meta[article:published_time] or <time>; fallback to Wayback timestamp.\"\"\"\n",
    "        meta = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            d = _only_date(meta.get(\"content\"))\n",
    "            if d:\n",
    "                return d\n",
    "        t = soup.find(\"time\")\n",
    "        if t:\n",
    "            d = _only_date(t.get(\"datetime\") or t.get_text(strip=True))\n",
    "            if d:\n",
    "                return d\n",
    "        return _only_date(wayback_ts)\n",
    "\n",
    "    def scrape_article_payload(self, wayback_url, raw_url, item_id, wayback_ts):\n",
    "        \"\"\"\n",
    "        Parse snapshot:\n",
    "          - title\n",
    "          - body text (joined <p>)\n",
    "          - published date\n",
    "          - subject (first path segment)\n",
    "          - images: FIRST <img> = cover, remaining = other\n",
    "          - image alts map\n",
    "          - by / located extraction (author/byline zones → fallback to title+first body chunk)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            resp = self.session.get(wayback_url, timeout=20)\n",
    "            resp.raise_for_status()\n",
    "            # prefer lxml; fallback to html.parser\n",
    "            parser = \"lxml\"\n",
    "            try:\n",
    "                import lxml  # noqa\n",
    "            except Exception:\n",
    "                parser = \"html.parser\"\n",
    "            soup = BeautifulSoup(resp.content, parser)\n",
    "\n",
    "            # title\n",
    "            h1 = soup.find(\"h1\") or soup.find(\"title\")\n",
    "            title = h1.get_text(strip=True) if h1 else \"\"\n",
    "\n",
    "            # bodies\n",
    "            paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "            body_text = \"\\n\".join(paragraphs)\n",
    "\n",
    "            # date\n",
    "            published_date = self._extract_published_date(soup, wayback_ts)\n",
    "\n",
    "            # collect all <img> in order; first is cover, rest are other\n",
    "            article_zone = soup.find(\"article\") or soup\n",
    "            all_imgs, seen = [], set()\n",
    "            for img in article_zone.find_all(\"img\"):\n",
    "                cls = \" \".join(img.get(\"class\") or []).lower()\n",
    "                # if class contain logo / banner / ad skipped\n",
    "                if any(k in cls for k in (\"logo\", \"banner\", \"ad\", \"advert\", \"social\")):\n",
    "                    continue\n",
    "                src = _norm_img_url(img.get(\"src\"))\n",
    "                if not src or src in seen:\n",
    "                    continue\n",
    "                all_imgs.append(src)\n",
    "                seen.add(src)\n",
    "            cover_urls = all_imgs[:1]\n",
    "            other_urls = all_imgs[1:]\n",
    "\n",
    "            # alts\n",
    "            img_alts = {}\n",
    "            for img in soup.find_all(\"img\"):\n",
    "                src = _norm_img_url(img.get(\"src\"))\n",
    "                if not src:\n",
    "                    continue\n",
    "                img_alts[src] = (img.get(\"alt\") or \"\").strip()\n",
    "\n",
    "            # by/located: search in byline zones first\n",
    "            meta_zone_texts = []\n",
    "            for sel in [\n",
    "                {\"name\": \"span\", \"class_\": re.compile(\"author|byline\")},\n",
    "                {\"name\": \"div\",  \"class_\": re.compile(\"author|byline\")},\n",
    "                {\"name\": \"p\",    \"class_\": re.compile(\"author|byline\")},\n",
    "            ]:\n",
    "                for el in soup.find_all(sel[\"name\"], class_=sel[\"class_\"]):\n",
    "                    meta_zone_texts.append(el.get_text(\" \", strip=True))\n",
    "            by, located = _extract_by_loc_text(\"  \".join(meta_zone_texts))\n",
    "\n",
    "            # fallback: try title + first body chunk if still empty\n",
    "            if not by and not located:\n",
    "                head_and_lead = (title + \" \" + \" \".join(paragraphs[:3]))[:800]\n",
    "                by, located = _extract_by_loc_text(head_and_lead)\n",
    "\n",
    "            subject_name = _first_path_segment(raw_url)\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"body_text\": body_text,\n",
    "                \"published_date\": published_date,\n",
    "                \"cover_urls\": cover_urls,\n",
    "                \"other_urls\": other_urls,\n",
    "                \"img_alts\": img_alts,\n",
    "                \"subject_name\": subject_name,\n",
    "                \"by\": by,\n",
    "                \"located\": located,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[failed capture] {wayback_url} → {e}\")\n",
    "            return None\n",
    "\n",
    "    def download_images(self, urls, item_id, label, start_at=1):\n",
    "        \"\"\"\n",
    "        Download images to images/{item_id}/img/\n",
    "        Filename format: {item_id}_{label}_{N}.{ext}\n",
    "          - cover uses start_at=1  → {item_id}_cover_1.png\n",
    "          - other can start at 2   → {item_id}_other_2.png (if a cover exists)\n",
    "        Returns: list of (url, filename)\n",
    "        \"\"\"\n",
    "        saved = []\n",
    "        item_img_dir = os.path.join(self.img_root_dir, item_id, \"img\")\n",
    "        os.makedirs(item_img_dir, exist_ok=True)\n",
    "\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            try:\n",
    "                ext = os.path.splitext(url)[1].split(\"?\")[0].lower()\n",
    "                if ext not in [\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".bmp\", \".svg\"]:\n",
    "                    ext = \".jpg\"\n",
    "                seq = start_at + i - 1\n",
    "                fname = f\"{item_id}_{label}_{seq}{ext}\"\n",
    "                fpath = os.path.join(item_img_dir, fname)\n",
    "\n",
    "                r = self.session.get(url, timeout=20)\n",
    "                if r.status_code == 200:\n",
    "                    with open(fpath, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    saved.append((url, fname))\n",
    "                else:\n",
    "                    print(f\"[Download {r.status_code}] {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Download error] {url} → {e}\")\n",
    "        return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4aa0929-7328-42ee-8f00-44bd59813b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- per-row ----------\n",
    "def process_row(row, scraper: WaybackScraper, skip_file_path: str):\n",
    "    raw_url = row[URL_FIELD]\n",
    "    item_id = str(row[ID_FIELD])\n",
    "\n",
    "    snap = scraper.get_latest_snapshot(raw_url)\n",
    "    if not snap:\n",
    "        with open(skip_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(raw_url + \"\\n\")\n",
    "        return None\n",
    "\n",
    "    parsed = scraper.scrape_article_payload(\n",
    "        wayback_url=snap['wayback_url'],\n",
    "        raw_url=raw_url,\n",
    "        item_id=item_id,\n",
    "        wayback_ts=snap['timestamp']\n",
    "    )\n",
    "    if not parsed:\n",
    "        with open(skip_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(raw_url + \"\\n\")\n",
    "        return None\n",
    "\n",
    "    # download images with desired numbering\n",
    "    cover_saved = scraper.download_images(parsed[\"cover_urls\"], item_id, \"cover\", start_at=1)\n",
    "    other_start = 2 if cover_saved else 1\n",
    "    other_saved = scraper.download_images(parsed[\"other_urls\"], item_id, \"other\", start_at=other_start)\n",
    "\n",
    "    # build associations: first = \"cover\"; others = \"other_1\", \"other_2\", ...\n",
    "    associations = []\n",
    "\n",
    "    def _mk_assoc(name_label, url_fname_tuple):\n",
    "        url, fname = url_fname_tuple\n",
    "        href = f\"./images/{item_id}/img/{fname}\"\n",
    "        ctype = _content_type_from_ext(fname)\n",
    "        return {\n",
    "            \"name\": name_label,\n",
    "            \"uri\": url,\n",
    "            \"type\": \"picture\",\n",
    "            \"headlines\": [{\"value\": parsed[\"img_alts\"].get(url, \"\")}],\n",
    "            \"renditions\": [{\"href\": href, \"contentType\": ctype}]\n",
    "        }\n",
    "\n",
    "    if cover_saved:\n",
    "        associations.append(_mk_assoc(\"cover\", cover_saved[0]))\n",
    "    for i, tup in enumerate(other_saved, 1):\n",
    "        associations.append(_mk_assoc(f\"other_{i}\", tup))\n",
    "\n",
    "    # dates\n",
    "    firstcreated = parsed[\"published_date\"] or _only_date(snap['timestamp'])\n",
    "    versioncreated = _only_date(snap['timestamp'])  # Wayback snapshot date\n",
    "    contentcreated = firstcreated\n",
    "    subjects = [{\"name\": parsed[\"subject_name\"]}] if parsed[\"subject_name\"] else []\n",
    "\n",
    "    ninjs_obj = {\n",
    "        \"uri\": raw_url,\n",
    "        \"standard\": {\n",
    "            \"name\": \"ninjs\",\n",
    "            \"version\": \"3.0\",\n",
    "            \"schema\": \"https://www.iptc.org/std/ninjs/ninjs-schema_3.0.json\"\n",
    "        },\n",
    "        \"firstcreated\": firstcreated,\n",
    "        \"versioncreated\": versioncreated,\n",
    "        \"contentcreated\": contentcreated,\n",
    "        \"type\": \"text\",\n",
    "        \"language\": \"zh-Hant-TW\",\n",
    "        \"headlines\": [{\"role\": \"main\", \"value\": parsed[\"title\"]}],\n",
    "        \"subjects\": subjects,\n",
    "        \"bodies\": [{\"role\": \"main\", \"contentType\": \"text/plain\", \"value\": parsed[\"body_text\"]}],\n",
    "        \"associations\": associations,\n",
    "        \"by\": parsed.get(\"by\", \"\"),          # <-- filled from extractor\n",
    "        \"located\": parsed.get(\"located\", \"\"),# <-- filled from extractor\n",
    "        \"altids\": [{\"role\": \"internal\", \"value\": item_id}]\n",
    "    }\n",
    "    return ninjs_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a226ed-7d83-4c8e-a6ae-b4ffcfc4eefc",
   "metadata": {},
   "source": [
    "## 主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "818b8e88-4ede-4939-af7e-73fe8b63bbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activetime: 2025-08-27 07:52:56\n",
      "output_index: output_20250827_075256\n",
      "[1] ✔ https://www.appledaily.com.tw/local/20220101/4C4Q3YRHTZGGBN2YAHLJH4CSQI/\n",
      "[2] ✔ https://www.appledaily.com.tw/local/20220101/DUS6LUBV2FEHBHZ5R47BG6YCCY/\n",
      "[3] ✔ https://www.appledaily.com.tw/local/20220101/23YFPLCSYZCZPANVI4E2LTW5L4/\n",
      "[4] ✔ https://www.appledaily.com.tw/local/20220101/C4AZKGYVYZDVTPWZ7RPWARUHDY/\n",
      "[CDX error] https://www.appledaily.com.tw/local/20220101/C5GNIZT4DNA4DAFUVA5NHKJYDI/ → HTTPSConnectionPool(host='web.archive.org', port=443): Read timed out. (read timeout=20)\n",
      "[5] ✘ skipped\n",
      "already output ninjs.json\n",
      "zipped: output_20250827_075256.zip\n",
      "total 1 URL skipped\n",
      "cost: 21.17 s\n"
     ]
    }
   ],
   "source": [
    "# ---------- main ----------\n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    print(\"activetime:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = f\"{OUTPUT_PREFIX}{ts}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    img_root_dir = os.path.join(out_dir, \"images\")\n",
    "    skip_file_path = os.path.join(out_dir, SKIP_FILE_NAME)\n",
    "    os.makedirs(img_root_dir, exist_ok=True)\n",
    "    if os.path.exists(skip_file_path):\n",
    "        os.remove(skip_file_path)\n",
    "\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    if N_LIMIT:\n",
    "        df = df.head(N_LIMIT)\n",
    "\n",
    "    print(\"output_index:\", out_dir)\n",
    "    scraper = WaybackScraper(img_root_dir=img_root_dir)\n",
    "\n",
    "    ninjs_list = []\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        tasks = {executor.submit(process_row, row, scraper, skip_file_path): row for _, row in df.iterrows()}\n",
    "        for i, f in enumerate(as_completed(tasks), 1):\n",
    "            obj = f.result()\n",
    "            if obj:\n",
    "                ninjs_list.append(obj)\n",
    "                print(f\"[{i}] ✔ {obj['uri']}\")\n",
    "            else:\n",
    "                print(f\"[{i}] ✘ skipped\")\n",
    "\n",
    "    # ninjs.json（array）\n",
    "    if ninjs_list:\n",
    "        with open(os.path.join(out_dir, \"ninjs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(ninjs_list, f, ensure_ascii=False, indent=2)\n",
    "        print(\"already output ninjs.json\")\n",
    "    else:\n",
    "        print(\"failed\")\n",
    "\n",
    "    # zip all files\n",
    "    zip_name = out_dir + \".zip\"\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "        for root, _, files in os.walk(out_dir):\n",
    "            for file in files:\n",
    "                fp = os.path.join(root, file)\n",
    "                z.write(fp, os.path.relpath(fp, start=os.path.dirname(out_dir)))\n",
    "    print(f\"zipped: {zip_name}\")\n",
    "\n",
    "    # skip count\n",
    "    skip_count = 0\n",
    "    if os.path.exists(skip_file_path):\n",
    "        with open(skip_file_path, encoding=\"utf-8\") as f:\n",
    "            skip_count = len([l for l in f if l.strip()])\n",
    "    print(f\"total {skip_count} URL skipped\")\n",
    "    print(\"cost: %.2f s\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108075d2-4117-4511-8cc2-92ede10f64d6",
   "metadata": {},
   "source": [
    "## 輸出結果\n",
    "activetime: 2025-08-26 20:36:48\n",
    "output_index: output_20250826_203648\n",
    "[1] ✔ https://www.appledaily.com.tw/local/20220101/4C4Q3YRHTZGGBN2YAHLJH4CSQI/\n",
    "[2] ✔ https://www.appledaily.com.tw/local/20220101/C4AZKGYVYZDVTPWZ7RPWARUHDY/\n",
    "[3] ✔ https://www.appledaily.com.tw/local/20220101/DUS6LUBV2FEHBHZ5R47BG6YCCY/\n",
    "[4] ✔ https://www.appledaily.com.tw/local/20220101/C5GNIZT4DNA4DAFUVA5NHKJYDI/\n",
    "[5] ✔ https://www.appledaily.com.tw/local/20220101/23YFPLCSYZCZPANVI4E2LTW5L4/\n",
    "already output ninjs.json\n",
    "zipped: output_20250826_203648.zip\n",
    "total 0 URL skipped\n",
    "cost: 44.86 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b4eca-1fb6-4a0f-9619-d94beb3e1ba8",
   "metadata": {},
   "source": [
    "## 檔案結構\n",
    "```\n",
    "├─ images/                          # 存放所有文章的圖片\n",
    "│  ├─ 4C4Q3Y...CSQI/\n",
    "│  │  └─ img/                       # 這篇文章的圖片子資料夾\n",
    "│  ├─ 23YFPL...TW5L4/\n",
    "│  │  └─ img/\n",
    "│  ├─ C4AZKG...RUHDY/\n",
    "│  │  └─ img/\n",
    "│  ├─ C5GNIZ...HJYDI/\n",
    "│  │  └─ img/\n",
    "│  └─ DUS6LU...6YCCY/\n",
    "│     └─ img/\n",
    "└─ ninjs.json                       # 匯出的新聞資料（JSON 陣列）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed7497-42a7-449b-9894-6bfa8f4a3cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b16e05-b9cd-4b4d-9a17-1e5cece25fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
